{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7ea2c99",
   "metadata": {},
   "source": [
    "# 01 â€” Feature Engineering (Market + Options Sentiment)\n",
    "\n",
    "This notebook builds a **feature-rich dataset** by combining:\n",
    "- technical indicators from market data (via `ta`), and\n",
    "- options-based sentiment features (put/call, OI skew, rolling stats).\n",
    "\n",
    "We also create lagged features and compute quick correlation & model-based feature importance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7af1f55",
   "metadata": {},
   "source": [
    "# Bootstrap: ensure the 'src' package path is visible from notebooks directory\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))\n",
    "\n",
    "from oami.config import initialize_environment\n",
    "api_key = initialize_environment()\n",
    "print('Environment initialized. API key status:', 'set' if api_key != 'YOUR_KEY_HERE' else 'not set')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee2b482",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from oami.data_layer import get_market_data, get_options_data\n",
    "\n",
    "DATA_DIR = Path('..') / 'data' / 'csv' / 'day'\n",
    "FEATURES_OUT = Path('..') / 'data' / 'features' / 'day'\n",
    "FEATURES_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SYMBOL = 'SPY'\n",
    "start, end = '2024-01-01', '2025-10-01'\n",
    "\n",
    "def load_or_fetch_market(symbol: str, start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    \"\"\"Load market data from cache or fetch via Polygon.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    symbol : str\n",
    "        Ticker symbol (e.g., 'SPY').\n",
    "    start_date : str\n",
    "        ISO date string for start.\n",
    "    end_date : str\n",
    "        ISO date string for end.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Market OHLCV with a Date column.\n",
    "    \"\"\"\n",
    "    path = DATA_DIR / f'{symbol}.csv'\n",
    "    if path.exists():\n",
    "        logging.info('Loading market data from cache: %s', path)\n",
    "        return pd.read_csv(path, parse_dates=['Date']).sort_values('Date')\n",
    "    logging.info('Fetching market data via Polygon: %s', symbol)\n",
    "    return get_market_data(symbol, start_date, end_date)\n",
    "\n",
    "def load_or_fetch_options(symbol: str, start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    \"\"\"Load options summary from cache or fetch via Polygon.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    symbol : str\n",
    "        Underlying ticker symbol.\n",
    "    start_date : str\n",
    "        Start date (ISO).\n",
    "    end_date : str\n",
    "        End date (ISO).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Daily options aggregates with sentiment fields if present.\n",
    "    \"\"\"\n",
    "    path = DATA_DIR / f'{symbol}_options.csv'\n",
    "    if path.exists():\n",
    "        logging.info('Loading options data from cache: %s', path)\n",
    "        return pd.read_csv(path, parse_dates=['Date']).sort_values('Date')\n",
    "    logging.info('Fetching options data via Polygon: %s', symbol)\n",
    "    return get_options_data(symbol, start_date, end_date)\n",
    "\n",
    "market_df = load_or_fetch_market(SYMBOL, start, end)\n",
    "options_df = load_or_fetch_options(SYMBOL, start, end)\n",
    "market_df.head(), options_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c094b767",
   "metadata": {},
   "source": [
    "from typing import Iterable, List\n",
    "import ta\n",
    "\n",
    "def build_ta_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create technical indicators using `ta` library.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Market OHLCV DataFrame with columns: Date, Open, High, Low, Close, Volume.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Copy of input with added indicator columns.\n",
    "    \"\"\"\n",
    "    out = df.copy().sort_values('Date')\n",
    "    c, h, l, v = out['Close'], out['High'], out['Low'], out['Volume']\n",
    "\n",
    "    # Trend & Momentum\n",
    "    for w in (5, 10, 20):\n",
    "        out[f'sma_{w}'] = c.rolling(w).mean()\n",
    "    for w in (10, 20):\n",
    "        out[f'ema_{w}'] = c.ewm(span=w, adjust=False).mean()\n",
    "    out['rsi_14'] = ta.momentum.RSIIndicator(close=c, window=14).rsi()\n",
    "    macd = ta.trend.MACD(close=c, window_fast=12, window_slow=26, window_sign=9)\n",
    "    out['macd'], out['macd_signal'], out['macd_hist'] = macd.macd(), macd.macd_signal(), macd.macd_diff()\n",
    "\n",
    "    # Volatility\n",
    "    bb = ta.volatility.BollingerBands(close=c, window=20, window_dev=2)\n",
    "    out['bb_width'] = bb.bollinger_wband()\n",
    "    out['atr_14'] = ta.volatility.AverageTrueRange(high=h, low=l, close=c, window=14).average_true_range()\n",
    "    for w in (10, 20):\n",
    "        out[f'roll_std_{w}'] = c.rolling(w).std()\n",
    "\n",
    "    # Volume features\n",
    "    out['vol_ma_10'] = v.rolling(10).mean()\n",
    "    out['vol_z_20'] = (v - v.rolling(20).mean()) / (v.rolling(20).std())\n",
    "\n",
    "    # Returns & target\n",
    "    out['ret_1'] = c.pct_change(1)\n",
    "    out['next_return'] = out['ret_1'].shift(-1)\n",
    "    return out\n",
    "\n",
    "def build_options_sentiment(df_opt: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create options sentiment features from options aggregates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_opt : pandas.DataFrame\n",
    "        Options summary with columns such as Date, PutVol, CallVol, PutOI, CallOI.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Options features aligned by Date with sentiment statistics.\n",
    "    \"\"\"\n",
    "    if df_opt is None or df_opt.empty:\n",
    "        return pd.DataFrame(columns=['Date'])\n",
    "    df = df_opt.copy().sort_values('Date')\n",
    "    # Ensure expected columns exist\n",
    "    for col in ['PutVol','CallVol','PutOI','CallOI']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    # Core ratios\n",
    "    df['pcr_vol'] = df['PutVol'] / df['CallVol'].replace({0: np.nan})\n",
    "    df['oi_skew'] = (df['CallOI'] - df['PutOI']) / (df['CallOI'] + df['PutOI']).replace({0: np.nan})\n",
    "    df['opt_total_vol'] = df['PutVol'].fillna(0) + df['CallVol'].fillna(0)\n",
    "\n",
    "    # Sentiment index: prefer provided, else derive as 1 - pcr\n",
    "    if 'SentimentIndex' in df.columns:\n",
    "        df['sentiment'] = df['SentimentIndex']\n",
    "    else:\n",
    "        df['sentiment'] = 1 - df['pcr_vol']\n",
    "\n",
    "    # Rolling stats\n",
    "    for w in (5, 10, 20):\n",
    "        df[f'pcr_rollmean_{w}'] = df['pcr_vol'].rolling(w).mean()\n",
    "        df[f'pcr_rollstd_{w}'] = df['pcr_vol'].rolling(w).std()\n",
    "        df[f'sent_rollmean_{w}'] = df['sentiment'].rolling(w).mean()\n",
    "        df[f'sent_rollstd_{w}'] = df['sentiment'].rolling(w).std()\n",
    "\n",
    "    # Optional IV rank if IV column is present\n",
    "    if 'IV' in df.columns:\n",
    "        iv = df['IV']\n",
    "        iv_min, iv_max = iv.rolling(252).min(), iv.rolling(252).max()\n",
    "        df['iv_rank_1y'] = (iv - iv_min) / (iv_max - iv_min)\n",
    "\n",
    "    return df\n",
    "\n",
    "mkt = build_ta_features(market_df)\n",
    "opt = build_options_sentiment(options_df)\n",
    "\n",
    "features = pd.merge(mkt, opt, on='Date', how='left')\n",
    "features.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03a95c33-d1f1-4724-b77c-7461024bacf8",
   "metadata": {},
   "source": [
    "get_market_data(\"SPY\", \"2024-01-01\", \"2025-10-01\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f345b69b",
   "metadata": {},
   "source": [
    "def make_lags(df: pd.DataFrame, cols: list, lags=(1,3,5,10)) -> pd.DataFrame:\n",
    "    \"\"\"Add lagged features for selected columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Input feature table with a `Date` column.\n",
    "    cols : list\n",
    "        Columns to lag.\n",
    "    lags : tuple, optional\n",
    "        Lags in days to create, by default (1, 3, 5, 10).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with lagged columns appended.\n",
    "    \"\"\"\n",
    "    out = df.copy().sort_values('Date')\n",
    "    for c in cols:\n",
    "        if c not in out.columns:\n",
    "            continue\n",
    "        for L in lags:\n",
    "            out[f'{c}_lag{L}'] = out[c].shift(L)\n",
    "    return out\n",
    "\n",
    "# Choose a focused set of columns for lagging\n",
    "lag_columns = [\n",
    "    'ret_1','rsi_14','macd','macd_signal','bb_width','atr_14',\n",
    "    'pcr_vol','oi_skew','sentiment'\n",
    "]\n",
    "features = make_lags(features, lag_columns)\n",
    "features = features.dropna().reset_index(drop=True)\n",
    "features.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ddaecab",
   "metadata": {},
   "source": [
    "# Save engineered dataset\n",
    "out_path = FEATURES_OUT / f'{SYMBOL}_features.csv'\n",
    "features.to_csv(out_path, index=False)\n",
    "print('Saved features to:', out_path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "676863bd",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "numeric = features.drop(columns=['Date']).select_dtypes(include=[np.number])\n",
    "corr = numeric.corr()\n",
    "\n",
    "# Plot a correlation heatmap (matplotlib only; single chart; default colormap)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(corr, aspect='auto')\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "plt.yticks(range(len(corr.index)), corr.index)\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5c70f48",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Clean feature matrix\n",
    "drop_cols = ['Open','High','Low','Close','Volume']\n",
    "X = features.drop(columns=[c for c in drop_cols if c in features.columns])\n",
    "X = X.drop(columns=['Date','next_return'], errors='ignore').select_dtypes(include=[np.number])\n",
    "\n",
    "# Target (optional: try next_5d_return instead)\n",
    "y = features['next_return']\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=500, max_depth=10, min_samples_leaf=5, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingRegressor(n_estimators=500, learning_rate=0.05, max_depth=5, random_state=42),\n",
    "    'Ridge': Ridge(alpha=0.1, fit_intercept=True)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    results[name] = r2_score(y_test, pred)\n",
    "    print(f\"{name:<18} RÂ² = {results[name]:.6f}\")\n",
    "\n",
    "pd.DataFrame({'R2': results})\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8e00d83",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "for name, imp in importances.items():\n",
    "    print(f'\\nTop features â€” {name}')\n",
    "    display(imp.to_frame('importance'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "047d9eea",
   "metadata": {},
   "source": [
    "## âœ… Summary\n",
    "\n",
    "- Built technical indicators (SMA/EMA/RSI/MACD/BB/ATR) and rolling stats.\n",
    "- Engineered options sentiment features (put/call ratio, OI skew, rolling means/stds).\n",
    "- Created lagged predictors and saved the unified dataset.\n",
    "- Computed correlations and model-based feature importances (RF/GB/Ridge).\n",
    "\n",
    "Next step: train, evaluate, and compare ML models in `02_modeling.ipynb` (train/validation splits, CV, and robust importance aggregation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
